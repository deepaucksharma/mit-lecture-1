{
  "id": "11-evolution",
  "title": "Single Master Evolution",
  "narrative": "Every architecture has an expiration date. GFS was brilliant—for its time, its scale, its workload. But systems grow. Google went from hundreds to hundreds of thousands of machines. From large files to billions of small ones. From MapReduce to dozens of use cases. The single Master, once elegant, became a bottleneck. Not because the design was wrong, but because the world changed. This is the arc of all systems: birth, growth, strain, breaking, rebirth.",
  "crystallizedInsight": "Every design decision is a bet on the future. Some bets age well. Some don't. Know when to fold.",
  "layout": {
    "type": "flow"
  },
  "nodes": [
    {
      "id": "Y2003",
      "type": "note",
      "label": "2003: Launch\\n100 clients\\n1TB, 1M files\\n1GB RAM"
    },
    {
      "id": "Y2006",
      "type": "note",
      "label": "2006: Growing\\n1K clients\\n1PB, 50M files\\n16GB RAM"
    },
    {
      "id": "Y2009",
      "type": "note",
      "label": "2009: Strain\\n10K clients\\n10PB, 500M files\\n64GB RAM"
    },
    {
      "id": "Y2012",
      "type": "note",
      "label": "2012: Breaking\\n100K clients\\n100PB, 1B files\\n96GB RAM"
    }
  ],
  "edges": [
    {
      "id": "e1",
      "from": "Y2003",
      "to": "Y2006",
      "kind": "control",
      "label": "10x growth"
    },
    {
      "id": "e2",
      "from": "Y2006",
      "to": "Y2009",
      "kind": "control",
      "label": "10x growth"
    },
    {
      "id": "e3",
      "from": "Y2009",
      "to": "Y2012",
      "kind": "control",
      "label": "10x growth"
    }
  ],
  "scenes": [
    {
      "id": "growth",
      "title": "Growth Timeline",
      "overlays": []
    },
    {
      "id": "bottlenecks",
      "title": "Bottleneck Analysis",
      "overlays": ["bottleneck-points"]
    },
    {
      "id": "solutions",
      "title": "Attempted Solutions",
      "overlays": ["optimization-attempts"]
    }
  ],
  "overlays": [
    {
      "id": "bottleneck-points",
      "caption": "The Breaking Points",
      "diff": {
        "add": {
          "nodes": [
            {
              "id": "evolution-insight",
              "type": "note",
              "label": "Systems don't fail suddenly\\nThey strain, creak, then break\\nEach phase teaches a lesson"
            }
          ]
        },
        "highlight": {
          "nodeIds": ["Y2006", "Y2009", "Y2012"]
        },
        "modify": {
          "nodes": [
            {
              "id": "Y2006",
              "label": "2006: RAM Pressure\\n\\nSymptom: Master using 15.8GB of 16GB\\nBottleneck: Metadata doesn't fit\\nQuick fix: Larger chunks (64MB→128MB)\\nReal fix: Add more RAM\\n\\nLesson: Buy time with hardware"
            },
            {
              "id": "Y2009",
              "label": "2009: CPU Bottleneck\\n\\nSymptom: Master at 90% CPU\\nBottleneck: Too many metadata ops\\nQuick fix: Batch operations, cache more\\nReal fix: Can't buy our way out anymore\\n\\nLesson: Optimizations have diminishing returns"
            },
            {
              "id": "Y2012",
              "label": "2012: Fundamental Limits\\n\\nSymptom: Master crashes hourly\\nBottleneck: Single machine can't handle load\\nQuick fix: None that work\\nReal fix: New architecture (Colossus)\\n\\nLesson: Sometimes you need a rewrite"
            }
          ]
        }
      }
    },
    {
      "id": "optimization-attempts",
      "caption": "The Optimization Journey",
      "diff": {
        "add": {
          "nodes": [
            {
              "id": "optimization-note",
              "type": "note",
              "label": "Each optimization buys time\\nBut you can't optimize forever\\nEventually: architectural change"
            }
          ]
        },
        "modify": {
          "nodes": [
            {
              "id": "Y2006",
              "label": "2006: Easy Wins\\n\\n• Increased chunk size\\n• Metadata compression\\n• Operation batching\\n• Lazy space reclamation\\n\\nResult: 3 more years of runway"
            },
            {
              "id": "Y2009",
              "label": "2009: Harder Trade-offs\\n\\n• Namespace sharding (complex)\\n• Aggressive metadata caching\\n• Reduced heartbeat frequency\\n• Shadow master load balancing\\n\\nResult: 3 more years, barely"
            },
            {
              "id": "Y2012",
              "label": "2012: Out of Options\\n\\n• Every optimization applied\\n• Hardware maxed out\\n• Complexity through the roof\\n• Operational burden crushing\\n\\nResult: Time to rebuild from scratch"
            }
          ]
        }
      }
    },
    {
      "id": "workload-shift",
      "caption": "The Workload That Changed Everything",
      "diff": {
        "add": {
          "nodes": [
            {
              "id": "workload-note",
              "type": "note",
              "label": "2003: MapReduce on crawl data\\n- Large files (GBs)\\n- Sequential access\\n- Few files per job\\n\\n2012: Gmail, Photos, Docs\\n- Small files (KBs)\\n- Random access\\n- Billions of files\\n\\nGFS optimized for 2003.\\n2012 broke all assumptions."
            }
          ]
        }
      }
    }
  ],
  "contracts": {
    "invariants": [
      "Single master simplifies consistency",
      "All metadata in RAM for performance",
      "64 bytes per chunk metadata overhead"
    ],
    "guarantees": [
      "Strong metadata consistency",
      "Fast metadata operations",
      "Simple failure recovery"
    ],
    "caveats": [
      "RAM limits file count",
      "CPU limits operation rate",
      "Single point of failure"
    ]
  },
  "drills": [
    {
      "id": "drill-memory-wall",
      "type": "analyze",
      "prompt": "Why does metadata have to live in RAM? Couldn't they use disk/SSD for overflow?",
      "thoughtProcess": [
        "Metadata operations must be FAST—single-digit milliseconds",
        "Option 1: Keep all metadata in RAM",
        "- Lookup: ~100ns (perfect)",
        "- Limit: RAM size caps the system",
        "Option 2: Spill to SSD",
        "- Lookup: ~100μs (1000x slower)",
        "- Every metadata op now 1000x slower",
        "- Client latency explodes, throughput tanks",
        "Option 3: Use SSD with caching",
        "- Cache hit: Fast (RAM)",
        "- Cache miss: Slow (SSD)",
        "- Working set matters—if it doesn't fit in cache, performance cliff",
        "Option 4: Distribute metadata (Colossus's choice)",
        "- Split metadata across many machines",
        "- Each machine has less to track, fits in RAM",
        "- Complexity increases, but scale improves",
        "GFS chose Option 1: Simple, fast, limited",
        "Colossus chose Option 4: Complex, fast, unlimited"
      ],
      "insight": "RAM is fast but finite. When you hit the limit, you either distribute or slow down. There's no third option."
    },
    {
      "id": "drill-billion-files",
      "type": "apply",
      "prompt": "Calculate: With 96GB RAM, how many files can GFS handle? Show the math.",
      "scenario": "96GB RAM, 64 bytes per chunk, average 3 chunks per file, namespace overhead 64 bytes per file",
      "thoughtProcess": [
        "Available memory: 96GB = 96 × 1024³ bytes = 103,079,215,104 bytes",
        "Per-file overhead:",
        "- Namespace entry: ~64 bytes (filename, permissions, etc.)",
        "- Average 3 chunks per file",
        "- Per chunk: 64 bytes (location, version, etc.)",
        "- Total per file: 64 + (3 × 64) = 256 bytes",
        "Maximum files: 103,079,215,104 ÷ 256 = ~402 million files",
        "But wait—Gmail has billions of users, each with thousands of emails",
        "That's trillions of files needed",
        "Even with 96GB RAM, GFS hits a hard wall at ~400M files",
        "This is why Colossus distributes metadata",
        "With 1000 metadata servers, each handling 400M files: 400 billion files possible",
        "Different architecture, different scale"
      ],
      "insight": "You can calculate the death of your architecture. The question is: do you calculate it before or after it happens?"
    },
    {
      "id": "drill-colossus-lesson",
      "type": "create",
      "prompt": "You're in 2009, presenting to leadership. Should Google invest in more GFS optimizations, or start building Colossus? Make your case.",
      "thoughtProcess": [
        "Argument for more GFS optimizations:",
        "- We know this codebase cold",
        "- Optimizations are low risk, proven to work",
        "- Colossus will take 2-3 years to build",
        "- What if we can squeeze another 5 years out of GFS?",
        "Argument for Colossus:",
        "- Every optimization makes GFS more complex",
        "- We're hitting fundamental limits, not implementation bugs",
        "- The workload is shifting (Gmail, Photos) in ways GFS can't handle",
        "- 2-3 years to build, but then we're future-proof for a decade",
        "- Single master is a single point of failure—we need to fix that anyway",
        "The real question: Are we patching a sinking ship, or building a new one?",
        "My recommendation: Start Colossus now, keep optimizing GFS in parallel",
        "Why: It's not an either/or. We need both to bridge the gap.",
        "Risk: Yes, it's expensive. But the alternative is hitting a wall in 2012."
      ],
      "insight": "The hardest technical decision: when to stop optimizing and start rebuilding. Wait too long, you're in crisis mode. Too early, you waste resources."
    },
    {
      "id": "drill-predictions",
      "type": "analyze",
      "prompt": "What did the GFS designers in 2003 get right about the future? What did they miss?",
      "thoughtProcess": [
        "What they got RIGHT:",
        "- Commodity hardware would stay cheap (correct)",
        "- Scale would keep growing (correct)",
        "- Failures would be common (correct)",
        "- Large-scale data processing is the future (correct)",
        "What they MISSED:",
        "- The shift from large files to billions of small files",
        "- Gmail, Google Photos didn't exist yet in 2003",
        "- The need for low-latency random access (not just batch processing)",
        "- That Google would grow 1000x, not 10x or 100x",
        "- The rise of SSDs (in 2003, only spinning disks existed)",
        "What's fascinating: They weren't wrong, the world changed",
        "GFS was perfect for 2003-2009 Google",
        "It wasn't 'bad design'—it was design with assumptions",
        "All designs have assumptions. Some age better than others."
      ],
      "insight": "You can't predict the future. But you can build systems that are easier to replace when your predictions fail."
    }
  ],
  "assessmentCheckpoints": [
    {
      "id": "understand-scaling-limits",
      "competency": "I can identify the scaling limits of architectural choices",
      "checkYourself": "What breaks first in GFS? RAM, CPU, network, or disk?",
      "mastery": "You calculate the expiration date of your designs upfront"
    },
    {
      "id": "understand-optimization-vs-redesign",
      "competency": "I know when to optimize and when to rebuild",
      "checkYourself": "How do you decide if you're patching or delaying the inevitable?",
      "mastery": "You recognize the signs that optimizations are hitting diminishing returns"
    },
    {
      "id": "understand-workload-evolution",
      "competency": "I see how changing workloads invalidate design assumptions",
      "checkYourself": "What workload changes would break your current projects?",
      "mastery": "You design with workload evolution in mind"
    },
    {
      "id": "understand-technical-debt",
      "competency": "I understand that complexity accumulates with each optimization",
      "checkYourself": "How many layers of bandaids until the system is unmaintainable?",
      "mastery": "You factor maintainability into architectural decisions"
    }
  ]
}
