{
  "id": "06-read-path",
  "title": "Read Path with Cache Lifecycle",
  "prerequisites": {
    "concepts": [
      "Control/data plane separation (Spec 00, 05)",
      "Master holds metadata, chunkservers hold data (Spec 04)",
      "Why Master bottleneck must be avoided (Spec 00, 04)"
    ],
    "checkYourself": "Can you explain why the Master never touches file data? Do you understand what happens when a single component becomes a bottleneck?"
  },
  "narrative": "Watch how caching transforms the system. Not for speed—the time saved is trivial (2ms out of 107ms). But for survival—without it, the Master drowns in requests. This is the dance between memory and scalability. Caching is load shedding, not optimization—it's about system survival, not user experience. In production: 90% cache hit rate reduces Master load by 10x, enabling it to handle 200-500 ops/sec sustainably.",
  "crystallizedInsight": "Cache for load reduction, not latency. The Master is precious—protect it. This is Little's Law in action: L = λW",
  "firstPrinciples": {
    "cachingTheory": {
      "hitRatio": "h = cache_hits / total_requests. Master load = (1-h) × request_rate",
      "workingSet": "Temporal locality: 90% of requests access 10% of data (Pareto principle)",
      "optimalCache": "Bélády's algorithm: evict item with longest reuse distance (theoretical optimum)",
      "practicalCache": "LRU approximation: O(1) operations, near-optimal for temporal locality"
    },
    "littlesLawApplication": {
      "formula": "L = λW where L = requests in system, λ = arrival rate, W = service time",
      "withoutCache": "L_master = 10K/s × 5ms = 50 concurrent requests",
      "withCache": "L_master = 1K/s × 5ms = 5 concurrent requests (90% cache hit)",
      "implication": "10x reduction in Master queue depth → 10x headroom for growth"
    },
    "probabilisticAnalysis": {
      "cacheInvalidation": "P(stale) = 1 - e^(-λt) where λ = change rate, t = TTL",
      "optimalTTL": "TTL* = argmin(miss_cost × P(miss) + stale_cost × P(stale))",
      "gfsChoice": "60s TTL: balances Master protection (high) vs staleness (acceptable)"
    }
  },
  "layout": {
    "type": "sequence",
    "numbered": true
  },
  "nodes": [
    {
      "id": "C",
      "type": "client",
      "label": "Client",
      "metadata": {
        "cacheSize": "100MB"
      }
    },
    {
      "id": "M",
      "type": "master",
      "label": "Master",
      "metadata": {
        "ram": "64GB",
        "qps": "10K"
      }
    },
    {
      "id": "CS4",
      "type": "chunkserver",
      "label": "Chunkserver 4",
      "metadata": {
        "version": 7,
        "chunks": 15000
      }
    }
  ],
  "edges": [
    {
      "id": "lookup-req",
      "from": "C",
      "to": "M",
      "kind": "control",
      "label": "lookup(file, chunk#3)",
      "metrics": {
        "size": "200B",
        "latency": "2-5ms"
      }
    },
    {
      "id": "lookup-resp",
      "from": "M",
      "to": "C",
      "kind": "control",
      "label": "locations: [CS1, CS4, CS9]",
      "metrics": {
        "size": "300B",
        "latency": "0.1ms"
      }
    },
    {
      "id": "data-req",
      "from": "C",
      "to": "CS4",
      "kind": "data",
      "label": "read(handle, 150M-150.1M)",
      "metrics": {
        "size": "100B",
        "latency": "5ms"
      }
    },
    {
      "id": "data-resp",
      "from": "CS4",
      "to": "C",
      "kind": "data",
      "label": "data[100KB]",
      "metrics": {
        "size": "100KB",
        "latency": "105ms",
        "throughput": "10MB/s"
      }
    }
  ],
  "scenes": [
    {
      "id": "cold-cache",
      "title": "Cold Cache",
      "overlays": [],
      "narrative": "First read requires Master lookup for chunk location"
    },
    {
      "id": "warm-cache",
      "title": "Warm Cache",
      "overlays": ["cache-hit"],
      "narrative": "Subsequent reads skip Master using cached locations"
    },
    {
      "id": "stale-cache",
      "title": "Expired Cache",
      "overlays": ["cache-expired"],
      "narrative": "After TTL expires, client must refresh location info"
    }
  ],
  "overlays": [
    {
      "id": "cache-hit",
      "caption": "Using cached chunk location",
      "diff": {
        "remove": {
          "edgeIds": ["lookup-req", "lookup-resp"]
        },
        "add": {
          "nodes": [
            {
              "id": "cache",
              "type": "note",
              "label": "Cache: chunk#3→[CS1,CS4,CS9]"
            }
          ]
        },
        "highlight": {
          "edgeIds": ["data-req", "data-resp"]
        }
      }
    },
    {
      "id": "cache-expired",
      "caption": "Cache TTL expired",
      "diff": {
        "modify": {
          "edges": [
            {
              "id": "lookup-req",
              "label": "lookup(file, chunk#3) [CACHE MISS]"
            }
          ]
        },
        "highlight": {
          "edgeIds": ["lookup-req", "lookup-resp"]
        }
      }
    }
  ],
  "contracts": {
    "invariants": [
      "Master never transfers file data (preserves control/data separation, Amdahl's Law constraint)",
      "Client always validates chunk version (stale replica detection via monotonic version numbers)",
      "Cache entries have bounded TTL (typically 60s, balances staleness vs Master load)"
    ],
    "guarantees": [
      "Master returns only current-version replicas (version tracking in master metadata)",
      "Read will succeed if any replica is available (3-way replication tolerates 2 failures)",
      "Cached locations valid within TTL window (bounded staleness guarantee)"
    ],
    "caveats": [
      "Stale reads possible from lagging replicas (eventual consistency model, replica may miss recent mutations)",
      "Cache invalidation is not immediate (TTL-based, not event-driven for scalability)",
      "Network partitions may prevent reaching all replicas (client retries with different replica, may fail if all unreachable)"
    ]
  },
  "drills": [
    {
      "id": "drill-cache-paradox",
      "type": "analyze",
      "prompt": "The cache saves only 2ms out of 107ms total. Why is this tiny improvement critical?",
      "thoughtProcess": [
        "2ms seems insignificant - less than 2% improvement",
        "But that's the wrong metric entirely",
        "Without cache: Every read hits the Master",
        "1000 clients = 1000 requests/second to Master",
        "With 90% cache: Only 100 requests/second",
        "The Master survives because most clients never talk to it",
        "Insight: It's not about the client's time, it's about the Master's life"
      ],
      "insight": "Small optimizations can have system-changing effects when multiplied"
    },
    {
      "id": "drill-master-death",
      "type": "apply",
      "prompt": "All client caches just expired simultaneously (maybe a time sync issue). What happens?",
      "scenario": "1000 clients, all caches invalid, all need metadata",
      "thoughtProcess": [
        "Instant thundering herd - 1000 simultaneous requests",
        "Master queue explodes",
        "Some clients timeout and retry - making it worse",
        "Master becomes unresponsive",
        "But wait - the data still flows! Chunkservers are fine",
        "Only new operations fail, existing transfers continue",
        "This is the beauty of separation"
      ],
      "insight": "Cache expiry must be staggered. Synchronized behavior kills distributed systems."
    },
    {
      "id": "drill-cache-philosophy",
      "type": "create",
      "prompt": "You need to decide: How long should cache entries live? What are you optimizing for?",
      "thoughtProcess": [
        "Too short (1 second): Might as well not cache",
        "Too long (1 hour): Stale data everywhere",
        "Just right (60 seconds): Why this number?",
        "- Long enough to help during bursts",
        "- Short enough to see changes reasonably soon",
        "- Matches human attention spans",
        "But really: Different data needs different TTLs",
        "Hot data: cache longer. Cold data: cache shorter."
      ],
      "insight": "There's no perfect cache duration, only good-enough for your use case"
    }
  ],
  "assessmentCheckpoints": [
    {
      "id": "understand-cache-purpose",
      "competency": "I understand caching is about protecting the Master, not speed",
      "checkYourself": "Could you explain why a 2% speed improvement is worth the complexity?",
      "mastery": "You see caching as load distribution, not optimization"
    },
    {
      "id": "understand-cache-lifecycle",
      "competency": "I can trace the three states: cold, warm, expired",
      "checkYourself": "Can you draw what happens in each state without looking?",
      "mastery": "You understand the rhythm of cache-based systems"
    },
    {
      "id": "understand-failure-isolation",
      "competency": "I see how Master failure doesn't stop data flow",
      "checkYourself": "What continues working when the Master dies?",
      "mastery": "You appreciate the power of separation"
    }
  ],
  "advancedConcepts": {
    "cachingStrategies": {
      "writeThrough": "Write to cache and backing store. GFS doesn't use—no Master data writes",
      "writeBack": "Write to cache, lazy flush. Too complex for metadata",
      "readThrough": "Cache fetches on miss. GFS uses for metadata",
      "sideCache": "Application manages cache. GFS client caches chunk locations"
    },
    "cacheCoherence": {
      "strongCoherence": "All caches see same value instantly. Too expensive at scale",
      "weakCoherence": "Caches may diverge temporarily. GFS uses with TTL bounds",
      "releaseConsistency": "Coherence at synchronization points. Not applicable to GFS",
      "eventualConsistency": "All caches converge eventually. GFS via TTL expiry"
    },
    "mathematicalModels": {
      "zipfDistribution": "P(rank k) ∝ 1/k^s. File access follows Zipf → caching very effective",
      "cachePartitioning": "Dedicated cache per workload vs shared. GFS uses per-client caches",
      "arcAlgorithm": "Adaptive Replacement Cache: balances recency and frequency",
      "scan-resistant": "Prevent sequential scans from polluting cache. GFS doesn't need—metadata small"
    },
    "modernEvolutions": {
      "distributedCaches": "Redis, Memcached: shared cache tier. Higher complexity",
      "cdnCaching": "Geographic distribution. Similar to GFS: metadata lookup → edge serve",
      "smartNics": "Cache in network cards. Bypasses CPU for common paths",
      "persistentMemory": "Intel Optane: cache survives restarts. Would help Master recovery"
    },
    "performanceAnalysis": {
      "amdahlImpact": "Cache reduces serial fraction: f = (1-h)×f_original",
      "queuingTheory": "M/M/1: Wait time = ρ/(1-ρ)/μ. Cache reduces ρ by factor of (1-h)",
      "costBenefit": "Cache value = miss_cost × hit_rate × request_rate - cache_maintenance_cost",
      "breakEvenPoint": "Min hit rate = cache_cost / (miss_cost × request_rate)"
    },
    "cacheInvalidation": {
      "ttlBased": "GFS approach: simple, predictable, but may serve stale",
      "eventBased": "Invalidate on change. Requires back-channel, complex at scale",
      "versionBased": "Include version in cache. Client validates. GFS uses for chunk versions",
      "leaseExtension": "Extend TTL on use. Keeps hot data cached longer"
    },
    "antiPatterns": {
      "cacheStampede": "Many clients miss simultaneously, overload backend. Use jittered TTL",
      "cacheChurn": "Constant eviction/refill. Size cache for working set",
      "negativeCaching": "Cache failures. GFS could cache 'file not found' to reduce Master load",
      "overCaching": "Cache everything. Complexity exceeds benefit. GFS caches only critical metadata"
    },
    "theoreticalLimits": {
      "idealCacheSize": "Working set size + ε. Beyond this, diminishing returns",
      "maxHitRate": "Limited by temporal locality. Typically 90-95% for metadata",
      "informationTheory": "Cache compression bounded by entropy of access pattern",
      "capTheorem": "Can't have consistent, available cache with partition tolerance"
    }
  }
}